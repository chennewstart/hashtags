package hashtags.bolt;

import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import storm.trident.operation.Function;
import storm.trident.operation.TridentCollector;
import storm.trident.operation.TridentOperationContext;
import storm.trident.tuple.TridentTuple;
import backtype.storm.tuple.Values;
import cern.colt.list.DoubleArrayList;
import cern.colt.list.IntArrayList;
import hashtags.utils.Tweet;
import hashtags.utils.SparseVector;

/**
 * 
 * @author Xiaohu Chen
 * 
 *         This is responsible for converting the tweet object into a sparse
 *         vector based on previously seen terms.
 * 
 */
public class Vectorizer implements Function {
	@Override
	public void prepare(Map conf, TridentOperationContext context) {
	}

	@Override
	public void execute(TridentTuple tuple, TridentCollector collector) {
		if (tuple.size() == 7) {
			// new Fields("tweet_id", "text", "hashtags", "words", "d", "df",
			// "pos")
			Long tweet_id = Long.valueOf(tuple.getString(0));
			String text = tuple.getString(1);
			Tweet tweet = new Tweet(tweet_id, text);
			// } catch (ClassCastException e) {
			// 	System.err.println("tuple:" + tuple);
			// 	System.err.println("tuple.getValue(0):" + tuple.getValue(0));
			// 	System.err.println(e);
			// }
			tweet.setHashtags((String) tuple.getValue(2));
			String tweetBody = tweet.getText();
			// java.util.ArrayList cannot be cast to [Ljava.lang.String;
			List<String> wordslist = (List<String>) tuple.getValue(3);
			String words[] = wordslist.toArray(new String[wordslist.size()]);
			Long d = (Long) tuple.getValue(4);
			List<Long> dflist = (List<Long>) tuple.getValue(5);
			Long df[] = dflist.toArray(new Long[dflist.size()]);
			List<Long> poslist = (List<Long>) tuple.getValue(6);
			Long pos[] = poslist.toArray(new Long[poslist.size()]);

			// if (words.length == 0) {
			// tweetBody = "ONLYLINKSANDMENTIONZ";
			// String dummyWord[] = { tweetBody };
			// words = dummyWord;
			// }
			collector.emit(getValues(tweet, words, d, df, pos));
		} else {
			// new Fields("text", "words", "d", "df", "pos"),
			String text = (String) tuple.getValue(0);
			// java.util.ArrayList cannot be cast to [Ljava.lang.String;
			List<String> wordslist = (List<String>) tuple.getValue(1);
			String words[] = wordslist.toArray(new String[wordslist.size()]);
			Long d = (Long) tuple.getValue(2);
			List<Long> dflist = (List<Long>) tuple.getValue(3);
			Long df[] = dflist.toArray(new Long[dflist.size()]);
			List<Long> poslist = (List<Long>) tuple.getValue(4);
			Long pos[] = poslist.toArray(new Long[poslist.size()]);

			collector.emit(getUnlabeledValues(text, words, d, df, pos));
		}
	}

	/**
	 * Normalization by dividing with Euclid norm.
	 * 
	 * @param vector
	 */
	public SparseVector normalizeVector(SparseVector vector) {
		// NORMALIZE HERE with norm1 so a unit vector is produced
		IntArrayList indexes = new IntArrayList(vector.cardinality());
		DoubleArrayList dbls = new DoubleArrayList(vector.cardinality());
		double norm = vector.getEuclidNorm();
		vector.getNonZeros(indexes, dbls);
		for (int i = 0; i < indexes.size(); i++) {
			vector.setQuick(indexes.get(i), dbls.getQuick(i) / norm);
		}
		return vector;
	}

	private Values getUnlabeledValues(String text, String[] words, Long d,
			Long df[], Long pos[]) {
		Map<String, Long> tf = new HashMap<String, Long>();
		for (String word : words) {
			Long cnt = tf.get(word);
			if (cnt == null) {
				tf.put(word, (long) 1);
			} else {
				tf.put(word, cnt + 1);
			}
		}

		int size = 0;

		for (int i = 0; i < pos.length; ++i) {
			if (pos[i] > size) {
				size = pos[i].intValue();
			}
		}

		Set<String> processed = new HashSet<String>();
		SparseVector vector = new SparseVector(size + 1);

		for (int i = 0; i < words.length; ++i) {
			String word = words[i];
			if (pos[i] < 0) {
				continue;
			}
			if (processed.contains(word)) {
				continue;
			}
			processed.add(word);
			vector.set(pos[i].intValue(), tf.get(word) * Math.log10(d)
					/ (df[i] + 1));
		}

		vector.trimToSize(); // very precious line, saves in performance

		vector = normalizeVector(vector);
		vector.trimToSize();

		// idfs have been updated when constructing the vector
		// tweet.setSparseVector(vector);
		return (new Values(vector));
	}

	private Values getValues(Tweet tweet, String[] words, Long d, Long df[],
			Long pos[]) {
		// DEBUG
		if (words.length != pos.length) {
			System.err.println("words(" + words.length + "):");
			for (int i = 0; i < words.length; ++i) {
				System.err.println(i + " " + words[i]);
			}

			System.err.println("pos(" + pos.length + "):");
			for (int i = 0; i < pos.length; ++i) {
				System.err.println(i + " " + pos[i]);
			}
		}
		Map<String, Long> tf = new HashMap<String, Long>();
		for (String word : words) {
			Long cnt = tf.get(word);
			if (cnt == null) {
				tf.put(word, (long) 1);
			} else {
				tf.put(word, cnt + 1);
			}
		}

		int uniqWordsIncrease = 0;
		int size = 0;

		for (int i = 0; i < pos.length; ++i) {
			if (pos[i] < 0) {
				uniqWordsIncrease++;
				pos[i] = -pos[i];
			}
			if (pos[i] > size) {
				size = pos[i].intValue();
			}
		}

		Set<String> processed = new HashSet<String>();
		SparseVector vector = new SparseVector(size + 1);

		for (int i = 0; i < words.length; ++i) {
			String word = words[i];
			if (processed.contains(word)) {
				continue;
			}
			processed.add(word);
			vector.set(pos[i].intValue(), tf.get(word) * Math.log10(d)
					/ (df[i] + 1));
		}

		vector.trimToSize(); // very precious line, saves in performance

		vector = normalizeVector(vector);
		vector.trimToSize();

		// idfs have been updated when constructing the vector
		tweet.setSparseVector(vector);
		return (new Values(tweet, uniqWordsIncrease));
	}

	@Override
	public void cleanup() {

	}
}
